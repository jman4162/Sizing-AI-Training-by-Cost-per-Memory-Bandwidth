{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jman4162/Sizing-AI-Training-by-Cost-per-Memory-Bandwidth/blob/main/Sizing_AI_Training_by_Cost_per_Memory_Bandwidth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b53b8aa6"
      },
      "source": [
        "# Sizing AI Training by **Cost per Memory Bandwidth**\n",
        "\n",
        "*A practical model (with math + Python) to tell if you’re compute-, memory-, or network-bound—and what to buy next*\n",
        "\n",
        "Author: John Hodge\n",
        "\n",
        "Date: 09/03/2025\n",
        "\n",
        "## Introduction\n",
        "\n",
        "When you’re training big transformers, the question that actually determines throughput isn’t “How many TFLOPs do I have?” It’s “How many **bytes per second** can I push through HBM, and at what **cost**?”\n",
        "\n",
        "This post gives you a compact, first-order model—both in math and in runnable Python—to:\n",
        "\n",
        "* Diagnose whether your run is **compute**, **memory**, or **network** bound,\n",
        "* Estimate tokens/sec per GPU and GPUs needed to hit a target,\n",
        "* Compare hardware using **\\$/TB/s/hour** (cost per memory bandwidth), which often tracks throughput/\\$ better than TFLOPs/\\$ for large LLM training.\n",
        "\n",
        "You’ll get tunable knobs for things like activation checkpointing, FlashAttention, and 8-bit optimizers, so you can adapt the model to your exact stack.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) The core idea\n",
        "\n",
        "Large-scale transformer training frequently hits the **memory wall**: step time is limited by how fast parameters, activations, and optimizer states move to/from HBM, not by peak FLOPs. That’s why the cost metric that matters is:\n",
        "\n",
        "$$\n",
        "\\textbf{Cost per memory bandwidth} \\;\\equiv\\;\n",
        "\\frac{\\$ / \\text{GPU-hour}}{\\text{HBM bandwidth (TB/s)}}\n",
        "\\quad\\Rightarrow\\quad \\frac{\\$}{\\text{TB/s·hour}}\n",
        "$$\n",
        "\n",
        "Lower is better. Hardware with high HBM BW (and good interconnect) can deliver more tokens/sec per dollar when your workload is bandwidth bound.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) A simple per-token model\n",
        "\n",
        "Let:\n",
        "\n",
        "* $N$: trainable parameters\n",
        "* $B_g$: **global tokens per step** (global batch × sequence length)\n",
        "* $\\kappa$: FLOPs/token coefficient (≈ **6** for forward+backward with Adam; ≈ **2** for forward only)\n",
        "* $\\gamma \\ge 1$: recompute multiplier (activation checkpointing overhead; e.g., **1.2–1.4**)\n",
        "* $\\alpha_{\\text{opt}}$: **optimizer traffic** in **bytes/param/step** (Adam bf16 often **16–20 B**; 8-bit Adam lower)\n",
        "* $L$: layers, $d_{\\text{model}}$: hidden size, $b$: **bytes/element** (2 for bf16/fp16)\n",
        "* $c_{\\text{act}}$: **activation traffic coefficient** (how many hidden-vectors’ worth of traffic per token per layer; lower with FlashAttention/fused kernels)\n",
        "\n",
        "Work per token (FLOPs)\n",
        "$$\n",
        "F_{\\text{tok}} \\;=\\; \\gamma \\cdot \\kappa \\cdot N\n",
        "$$\n",
        "HBM bytes per token\n",
        "$$\n",
        "M_{\\text{tok}} \\;=\\;\n",
        "\\underbrace{\\frac{\\alpha_{\\text{opt}}\\,N}{B_g}}_{\\text{optimizer + grads (amortized)}}\n",
        "\\;+\\;\n",
        "\\underbrace{c_{\\text{act}}\\;L\\,d_{\\text{model}}\\,b}_{\\text{activations}}\n",
        "$$\n",
        "Arithmetic intensity and “machine balance”\n",
        "$$\n",
        "I \\;=\\; \\frac{F_{\\text{tok}}}{M_{\\text{tok}}}\\quad(\\text{FLOPs/byte}),\n",
        "\\qquad\n",
        "\\text{Machine balance} \\;=\\; \\frac{C_{\\max}}{W_{\\text{HBM}}}\n",
        "$$\n",
        "where $C_{\\max}$ is the GPU’s usable FLOPs/s and $W_{\\text{HBM}}$ is its HBM bytes/s.\n",
        "\n",
        "If $ I <$ machine balance, you’re **memory-bound**; otherwise you’re **compute-bound**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) From per-token to tokens/sec per GPU\n",
        "\n",
        "Given hardware per GPU:\n",
        "\n",
        "* Compute: $C_{\\max}$ FLOPs/s\n",
        "* HBM BW: $W_{\\text{HBM}}$ bytes/s\n",
        "* Utilization $u \\in [0.6, 0.9]$ to capture kernel overlap, scheduling, etc.\n",
        "\n",
        "Then\n",
        "$$\n",
        "r_{\\text{comp}} = \\frac{C_{\\max}}{F_{\\text{tok}}},\\qquad\n",
        "r_{\\text{mem}}  = \\frac{W_{\\text{HBM}}}{M_{\\text{tok}}},\\qquad\n",
        "r_{\\text{gpu}}  = u \\cdot \\min(r_{\\text{comp}}, r_{\\text{mem}})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Add the network term (data parallel)\n",
        "\n",
        "All-reduce traffic for data-parallel gradients adds a per-token network cost (per GPU):\n",
        "$$\n",
        "M_{\\text{tok}}^{\\text{net}} \\;\\approx\\; \\frac{2\\,N\\,b_g}{B_g}\n",
        "\\quad\\Rightarrow\\quad\n",
        "r_{\\text{net}} \\;=\\; \\frac{W_{\\text{NIC}}}{M_{\\text{tok}}^{\\text{net}}}\n",
        "$$\n",
        "where $b_g$ is bytes/grad element (2 for bf16) and $W_{\\text{NIC}}$ is effective inter-node BW per GPU.\n",
        "\n",
        "Final bound:\n",
        "$$\n",
        "r_{\\text{gpu}} = u \\cdot \\min(r_{\\text{comp}},\\, r_{\\text{mem}},\\, r_{\\text{net}})\n",
        "$$\n",
        "Reading the tea leaves\n",
        "\n",
        "If **network-bound**, increase $B_g$ (bigger global batch), reduce DP shards (use TP/PP/ZeRO), or increase effective NIC BW (EFA/IB, overlap, grad compression).\n",
        "If **memory-bound**, raise arithmetic intensity: FlashAttention, fused kernels, selective recompute, 8-bit optimizer/state, larger $B_g$.\n",
        "If **compute-bound**, you’re doing great—push higher clocks/utilization or add parallelism.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) From per-GPU to cluster sizing\n",
        "\n",
        "For a target throughput $R_{\\text{target}}$ tokens/s:\n",
        "$$\n",
        "G \\;=\\; \\left\\lceil \\frac{R_{\\text{target}}}{r_{\\text{gpu}}} \\right\\rceil\n",
        "$$\n",
        "Approximate cluster HBM needed:\n",
        "$$\n",
        "\\text{Cluster TB/s} \\;\\approx\\; \\frac{R_{\\text{target}} \\cdot M_{\\text{tok}}}{10^{12}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Converting to cost per bandwidth\n",
        "\n",
        "For each candidate GPU:\n",
        "$$\n",
        "\\frac{\\$}{\\text{TB/s·hr}} \\;=\\; \\frac{\\$ / \\text{GPU-hr}}{\\text{HBM TB/s per GPU}}\n",
        "$$\n",
        "Choose the configuration that:\n",
        "\n",
        "* Meets $R_{\\text{target}}$ without being network-bound,\n",
        "* Minimizes \\$/TB/s·hr while maintaining enough compute balance.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) The Python reference implementation\n",
        "\n",
        "Below is a ready-to-use script that encodes the model. Plug in your own model/hardware numbers and compare options. (Numbers shown are placeholders—replace with your real specs and prices.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from math import ceil\n",
        "\n",
        "@dataclass\n",
        "class Hardware:\n",
        "    name: str\n",
        "    peak_flops_tflops: float     # usable BF16/FP16 TFLOPs per GPU\n",
        "    hbm_tbps: float              # HBM/GDDR TB/s per GPU\n",
        "    nic_gbps: float              # effective inter-node BW per GPU (Gb/s); 0 if none\n",
        "    price_per_gpu_hr: float      # $/GPU-hour\n",
        "    utilization: float = 0.75\n",
        "\n",
        "    @property\n",
        "    def machine_balance_flops_per_byte(self):\n",
        "        return (self.peak_flops_tflops * 1e12) / (self.hbm_tbps * 1e12)\n",
        "\n",
        "@dataclass\n",
        "class Model:\n",
        "    n_params: float              # trainable params\n",
        "    layers: int\n",
        "    d_model: int\n",
        "    bytes_per_elem: int = 2      # 2 for bf16/fp16\n",
        "\n",
        "@dataclass\n",
        "class TrainingCfg:\n",
        "    k_flops_per_token: float = 6.0     # ≈6*N for Adam (fwd+bwd)\n",
        "    recompute_mult: float = 1.0        # ≥1.0 (activation checkpointing)\n",
        "    alpha_opt_bytes_per_param: float = 16.0  # Adam traffic B/param/step\n",
        "    c_act: float = 6.0                 # activation traffic coefficient\n",
        "    global_tokens_per_step: int = 512_000    # global_batch * seq_len\n",
        "    bytes_per_grad_elem: int = 2\n",
        "\n",
        "def per_token_flops(model: Model, train: TrainingCfg) -> float:\n",
        "    return train.recompute_mult * train.k_flops_per_token * model.n_params\n",
        "\n",
        "def per_token_hbm_bytes(model: Model, train: TrainingCfg) -> float:\n",
        "    opt_bytes = (train.alpha_opt_bytes_per_param * model.n_params) / train.global_tokens_per_step\n",
        "    act_bytes = train.c_act * model.layers * model.d_model * model.bytes_per_elem\n",
        "    return opt_bytes + act_bytes\n",
        "\n",
        "def per_token_net_bytes(model: Model, train: TrainingCfg, dp_world_size: int) -> float:\n",
        "    if dp_world_size <= 1:\n",
        "        return 0.0\n",
        "    per_step = 2.0 * model.n_params * train.bytes_per_grad_elem\n",
        "    return per_step / train.global_tokens_per_step\n",
        "\n",
        "def tokens_per_sec_per_gpu(hw: Hardware, model: Model, train: TrainingCfg, dp_world_size: int = 1) -> dict:\n",
        "    F_tok = per_token_flops(model, train)\n",
        "    M_tok = per_token_hbm_bytes(model, train)\n",
        "    M_tok_net = per_token_net_bytes(model, train, dp_world_size)\n",
        "\n",
        "    r_comp = (hw.peak_flops_tflops * 1e12) / F_tok\n",
        "    r_mem  = (hw.hbm_tbps * 1e12)        / M_tok\n",
        "    if M_tok_net > 0 and hw.nic_gbps > 0:\n",
        "        nic_Bps = hw.nic_gbps * 1e9 / 8.0\n",
        "        r_net = nic_Bps / M_tok_net\n",
        "    else:\n",
        "        r_net = float('inf')\n",
        "\n",
        "    r_gpu = hw.utilization * min(r_comp, r_mem, r_net)\n",
        "    which_bound = [\"compute\",\"memory\",\"network\"][ [r_comp, r_mem, r_net].index(min(r_comp, r_mem, r_net)) ]\n",
        "    return dict(\n",
        "        r_gpu=r_gpu, r_comp=r_comp, r_mem=r_mem, r_net=r_net, bound=which_bound,\n",
        "        intensity=(F_tok/M_tok), machine_balance=hw.machine_balance_flops_per_byte\n",
        "    )\n",
        "\n",
        "def plan_cluster(hw: Hardware, model: Model, train: TrainingCfg, tokens_per_sec_target: float, dp_world_size:int=1):\n",
        "    rates = tokens_per_sec_per_gpu(hw, model, train, dp_world_size)\n",
        "    gpus = max(1, ceil(tokens_per_sec_target / rates[\"r_gpu\"]))\n",
        "    cost_per_hr = gpus * hw.price_per_gpu_hr\n",
        "    cluster_tbs = (tokens_per_sec_target * per_token_hbm_bytes(model, train)) / 1e12\n",
        "    return dict(\n",
        "        hardware=hw.name, gpus=gpus, per_hr=round(cost_per_hr,2),\n",
        "        per_gpu_tokens_s=round(rates[\"r_gpu\"],2), bound=rates[\"bound\"],\n",
        "        cluster_HBM_TBs=round(cluster_tbs,3),\n",
        "        per_TBs_hr=round(hw.price_per_gpu_hr / hw.hbm_tbps, 2)\n",
        "    )\n",
        "\n",
        "# --- Example catalog (replace with your real specs/prices) ---\n",
        "h100 = Hardware(name=\"H100-SXM\", peak_flops_tflops=900, hbm_tbps=3.35, nic_gbps=200, price_per_gpu_hr=3.93)\n",
        "h200 = Hardware(name=\"H200\",     peak_flops_tflops=1000, hbm_tbps=4.80, nic_gbps=400, price_per_gpu_hr=4.33)\n",
        "l4   = Hardware(name=\"L4\",       peak_flops_tflops=120,  hbm_tbps=0.30, nic_gbps=100, price_per_gpu_hr=1.15)\n",
        "\n",
        "# --- Example model/run (tune to your case) ---\n",
        "model = Model(n_params=70e9, layers=80, d_model=8192, bytes_per_elem=2)  # 70B decoder\n",
        "train  = TrainingCfg(\n",
        "    k_flops_per_token=6.0,  # Adam training\n",
        "    recompute_mult=1.2,     # some checkpointing\n",
        "    alpha_opt_bytes_per_param=16.0,\n",
        "    c_act=5.0,              # FlashAttention/fused kernels\n",
        "    global_tokens_per_step=512_000,\n",
        "    bytes_per_grad_elem=2\n",
        ")\n",
        "\n",
        "def compare_options(target_tokens_s=200_000, dp_world_size=8):\n",
        "    out = []\n",
        "    for hw in [h100, h200, l4]:\n",
        "        out.append(plan_cluster(hw, model, train, target_tokens_s, dp_world_size))\n",
        "    return sorted(out, key=lambda x: x[\"per_hr\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from pprint import pprint\n",
        "    pprint(compare_options())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QApAcUJv9_hL",
        "outputId": "989fcf82-d954-40f3-8f09-e99131afbc8b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'bound': 'compute',\n",
            "  'cluster_HBM_TBs': 1.748,\n",
            "  'gpus': 135,\n",
            "  'hardware': 'H200',\n",
            "  'per_TBs_hr': 0.9,\n",
            "  'per_gpu_tokens_s': 1488.1,\n",
            "  'per_hr': 584.55},\n",
            " {'bound': 'compute',\n",
            "  'cluster_HBM_TBs': 1.748,\n",
            "  'gpus': 150,\n",
            "  'hardware': 'H100-SXM',\n",
            "  'per_TBs_hr': 1.17,\n",
            "  'per_gpu_tokens_s': 1339.29,\n",
            "  'per_hr': 589.5},\n",
            " {'bound': 'compute',\n",
            "  'cluster_HBM_TBs': 1.748,\n",
            "  'gpus': 1120,\n",
            "  'hardware': 'L4',\n",
            "  'per_TBs_hr': 3.83,\n",
            "  'per_gpu_tokens_s': 178.57,\n",
            "  'per_hr': 1288.0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 8) Tuning the knobs (what the coefficients mean)\n",
        "\n",
        "$\\alpha_{\\text{opt}}$ (**bytes/param/step**)\n",
        "\n",
        "* Captures parameter, gradient, and optimizer state traffic.\n",
        "* Adam bf16/fp16: often **16–20 B**\n",
        "* 8-bit Adam / sharded optimizers: lower (tune from logs)\n",
        "\n",
        "$c_{\\text{act}}$ (**activation traffic**)\n",
        "\n",
        "* Encodes how IO-aware your kernels are.\n",
        "* Baseline attention: higher $c_{\\text{act}}$\n",
        "* FlashAttention + fused MLP/LayerNorm: lowers $c_{\\text{act}}$\n",
        "\n",
        "$\\gamma$ (**recompute multiplier**)\n",
        "\n",
        "* Activation checkpointing trades FLOPs for less memory. Expect **1.1–1.4** depending on policy.\n",
        "\n",
        "$B_g$ (**global tokens/step**)\n",
        "\n",
        "* Increasing $B_g$ amortizes optimizer and network bytes per token, often moving you from network/memory bound toward compute bound—until optimizer dynamics/generalization limit further growth.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Common failure modes (and fixes)\n",
        "\n",
        "- **Network-bound**: bound == \"network\": Increase $B_g$; reduce pure DP (add TP/PP, ZeRO), overlap comms, or boost effective NIC (EFA/IB, topology-aware placement).\n",
        "\n",
        "- **Memory-bound**: bound == \"memory\": Reduce bytes/token: FlashAttention/Flash-decoding, fused kernels, selective recompute, lower-precision optimizer/state, MoE (if appropriate), or simply pick GPUs with better \\$/TB/s·hr.\n",
        "\n",
        "- **Compute-bound** but low tokens/sec: Your utilization factor is pessimistic. Improve kernels (Flash/Flash-MLA, fused ops), overlap (streams), and ensure you’re not secretly constrained by PCIe/NVLink or CPU input pipelines.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Extensions\n",
        "\n",
        "* Inference: Set $\\kappa \\approx 2$, $\\alpha_{\\text{opt}}=0$, and replace activation term with KV-cache bytes/token.\n",
        "* MoE: Replace $N$ with the active parameters per token; keep memory/network terms aligned with your routing fraction and expert parallelism.\n",
        "* Long context: $c_{\\text{act}}$ rises; IO-aware attention matters even more.\n",
        "* Optimizer offload: Reduce $\\alpha_{\\text{opt}}$ but watch network/PCIe traffic tradeoffs.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Training frontier-scale models efficiently is mostly about buying and feeding the right bandwidth—on the GPU (HBM), across GPUs (NVLink/NVSwitch), and across nodes (NIC/fabric). The simple model above lets you quantify that trade space, decide whether you’re compute, memory, or network bound, and choose the cheapest TB/s that achieves your tokens/sec target.\n",
        "\n",
        "If you want, share your model size, sequence length, global batch, optimizer, and two or three candidate GPU types. I’ll plug them into this model and hand back a concrete capacity and cost comparison."
      ],
      "metadata": {
        "id": "7x4PifwN-ACP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "* **Roofline model (compute vs. memory bound):** Williams, Waterman, Patterson, *CACM* (2009). ([ACM Digital Library][1])\n",
        "* **FlashAttention (I/O-aware attention):** Dao et al., *arXiv:2205.14135* (2022). ([arXiv][2])\n",
        "* **FLOPs per token ≈ 6·N (training rule of thumb):** *How To Scale Your Model* (JAX-ML book), “All the transformer math you need to know.” ([jax-ml.github.io][3])\n",
        "* **Megatron-LM (scaling & comms patterns):** Shoeybi et al., *arXiv:1909.08053*; NVIDIA Megatron-LM repo notes. ([arXiv][4], [GitHub][5])\n",
        "* **Data-parallel gradient all-reduce (collective definition & cost intuition):** NCCL User Guide (AllReduce); UCSD notes on reduce-scatter + all-gather. ([NVIDIA Docs][6], [Hao AI Lab][7])\n",
        "* **ZeRO (optimizer/activation sharding to cut memory traffic):** Rajbhandari et al., *SC20* / arXiv:1910.02054. ([arXiv][8], [aiichironakano][9])\n",
        "* **8-bit optimizers (reduce optimizer-state bytes):** Dettmers et al., *arXiv:2110.02861*; bitsandbytes docs. ([arXiv][10], [Hugging Face][11])\n",
        "* **HBM bandwidth figures (for \\$/TB/s·hr):**\n",
        "\n",
        "  * NVIDIA **H100** datasheet—HBM3 up to **3.35 TB/s**. ([Megware][12])\n",
        "  * NVIDIA **H200**—HBM3e **4.8 TB/s**. ([NVIDIA][13])\n",
        "  * NVIDIA **B200**—HBM3e up to **8.0 TB/s**. ([primeline-solutions.com][14])\n",
        "  * NVIDIA **L4**—GDDR6 **300 GB/s**. ([NVIDIA][15])\n",
        "* **AWS EFA + NCCL (networking for multi-node training):** AWS EFA + NCCL guide; aws-ofi-nccl plugin. ([AWS Documentation][16], [GitHub][17])\n",
        "* **AWS UltraClusters / Capacity Blocks (cluster placement & pricing examples):** Capacity Blocks overview & pricing tables (p5/H100, p5e/H200). ([AWS Documentation][18], [Amazon Web Services, Inc.][19])\n",
        "* **P6-B200 (Blackwell) instance announcement & specs:** AWS blog (May 15, 2025). ([Amazon Web Services, Inc.][20])\n",
        "\n",
        "[1]: https://dl.acm.org/doi/10.1145/1498765.1498785 \"Roofline: an insightful visual performance model for multicore ...\"\n",
        "[2]: https://arxiv.org/abs/2205.14135 \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\"\n",
        "[3]: https://jax-ml.github.io/scaling-book/transformers/ \"All the Transformer Math You Need to Know | How To Scale ...\"\n",
        "[4]: https://arxiv.org/pdf/1909.08053 \"Megatron-LM: Training Multi-Billion Parameter Language ...\"\n",
        "[5]: https://github.com/NVIDIA/Megatron-LM \"NVIDIA/Megatron-LM: Ongoing research training ...\"\n",
        "[6]: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html \"Collective Operations — NCCL 2.27.5 documentation\"\n",
        "[7]: https://hao-ai-lab.github.io/dsc204a-w24/assets/scribe_notes/Feb_5_scribe_note.pdf \"13: Collective Communication 2 - Hao AI Lab @ UCSD\"\n",
        "[8]: https://arxiv.org/abs/1910.02054 \"ZeRO: Memory Optimizations Toward Training Trillion ...\"\n",
        "[9]: https://aiichironakano.github.io/cs596/Rajbhandari-ZeRO-SC20.pdf \"ZeRO: Memory Optimizations Toward Training Trillion ...\"\n",
        "[10]: https://arxiv.org/abs/2110.02861 \"8-bit Optimizers via Block-wise Quantization\"\n",
        "[11]: https://huggingface.co/docs/bitsandbytes/main/en/optimizers \"8-bit optimizers\"\n",
        "[12]: https://www.megware.com/fileadmin/user_upload/LandingPage%20NVIDIA/nvidia-h100-datasheet.pdf \"NVIDIA H100 Tensor Core GPU Datasheet\"\n",
        "[13]: https://www.nvidia.com/en-us/data-center/h200/ \"NVIDIA H200 Tensor Core GPU\"\n",
        "[14]: https://www.primeline-solutions.com/media/categories/server/nach-gpu/nvidia-hgx-h200/nvidia-blackwell-b200-datasheet.pdf \"nvidia-blackwell-b200-datasheet.pdf\"\n",
        "[15]: https://www.nvidia.com/en-us/data-center/l4/ \"L4 Tensor Core GPU for AI & Graphics\"\n",
        "[16]: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start-nccl.html \"Get started with EFA and NCCL for ML workloads on Amazon ...\"\n",
        "[17]: https://github.com/aws/aws-ofi-nccl \"aws/aws-ofi-nccl: This is a plugin which lets EC2 ...\"\n",
        "[18]: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-blocks.html \"Capacity Blocks for ML - Amazon Elastic Compute Cloud\"\n",
        "[19]: https://aws.amazon.com/ec2/capacityblocks/pricing/ \"Amazon EC2 Capacity Blocks for ML Pricing\"\n",
        "[20]: https://aws.amazon.com/blogs/aws/new-amazon-ec2-p6-b200-instances-powered-by-nvidia-blackwell-gpus-to-accelerate-ai-innovations/ \"New Amazon EC2 P6-B200 instances powered by NVIDIA ...\"\n"
      ],
      "metadata": {
        "id": "cu4fnDbdANiL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}