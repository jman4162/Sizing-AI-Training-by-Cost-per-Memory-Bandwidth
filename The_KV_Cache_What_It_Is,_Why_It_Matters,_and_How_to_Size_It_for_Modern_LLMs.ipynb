{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWrQi2PW0iqTKg7dnRd2vv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jman4162/Sizing-AI-Training-by-Cost-per-Memory-Bandwidth/blob/main/The_KV_Cache_What_It_Is%2C_Why_It_Matters%2C_and_How_to_Size_It_for_Modern_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The KV Cache: What It Is, Why It Matters, and How to Size It for Modern LLMs\n",
        "\n",
        "Author: John Hodge\n",
        "\n",
        "Date: 09/03/2025\n",
        "\n",
        "*If you only remember one thing:* During generation, large language models are often **memory-bandwidth bound**, not compute-bound. The **Key/Value (KV) cache** is the reason: it turns decoding into a fast lookup problem but makes **HBM bandwidth** and **VRAM** the dominant resources. Mastering KV cache mechanics is table stakes for building fast, cost-efficient LLM systems.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Quick intuition\n",
        "\n",
        "In decoder-only transformers, each new token must attend to **all prior tokens**. Naïvely, you’d recompute keys and values (K, V) for the entire prefix at every step—wasting tons of compute. Instead, we compute K and V **once** and **cache** them per layer. On the next token:\n",
        "\n",
        "1. Compute the new query **Qₜ**\n",
        "2. **Read** cached **K₁…ₜ₋₁, V₁…ₜ₋₁**\n",
        "3. Do attention → logits\n",
        "4. **Append** **Kₜ, Vₜ** to the cache for future steps\n",
        "\n",
        "**Result:** Huge compute savings, but your performance now hinges on how quickly you can **read** (and grow) that cache.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Prefill vs. Decode (the two phases)\n",
        "\n",
        "* **Prefill (prompt processing):** Run the model on the entire prompt once to build the cache. This is relatively **compute-heavy** and benefits from high TFLOPs.\n",
        "* **Decode (token-by-token):** For each generated token, read all prior K/V from cache. This phase is typically **memory-bandwidth bound** (HBM GB/s or TB/s dominates).\n",
        "\n",
        "Throughput (tokens/s) in decode falls as context grows because you read more K/V every step.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) How big is the KV cache?\n",
        "\n",
        "For one token, one layer:\n",
        "\n",
        "$$\n",
        "\\text{K/V bytes per token per layer} = 2 \\times n_{\\text{kv\\_heads}} \\times d_{\\text{head}} \\times \\text{bytes\\_per\\_elem}\n",
        "$$\n",
        "\n",
        "Across all layers:\n",
        "\n",
        "$$\n",
        "\\text{KV bytes per token} \\approx 2 \\times L \\times n_{\\text{kv\\_heads}} \\times d_{\\text{head}} \\times \\text{bytes}\n",
        "$$\n",
        "\n",
        "**Example (LLaMA-ish 70B):** $L=80$, $n_{\\text{heads}}=64$, $d_{\\text{head}}=128$, bf16 → 2 bytes/elem\n",
        "\n",
        "* **Standard MHA** (KV heads = 64):\n",
        "  $2 \\times 80 \\times 64 \\times 128 \\times 2$ ≈ **2.5 MB per token**\n",
        "  2k tokens → **\\~5 GB per sequence** (KV only).\n",
        "* **GQA** (e.g., 8 KV heads): **\\~320 KB/token** → **\\~0.64 GB** at 2k tokens.\n",
        "* **MQA** (1 KV head): **\\~40 KB/token** → **\\~80 MB** at 2k tokens.\n",
        "\n",
        "> **GQA/MQA** are massive wins for long context and batching: they shrink KV by **8–64×**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Bandwidth: why decode is memory-bound\n",
        "\n",
        "At decode step $t$, attention must **read** K/V for the first $t-1$ tokens (per layer). Per-token read grows \\~linearly with context length. Even with efficient kernels, the dominant cost is **bytes moved** from HBM:\n",
        "\n",
        "* **Arithmetic intensity** (FLOPs per byte moved) is **low** in decode.\n",
        "* Faster TFLOPs don’t help if HBM can’t feed the GPU—**you’re memory-bandwidth bound**.\n",
        "\n",
        "That’s why **“cost per memory bandwidth”** (e.g., **\\$/TB/s·hour**) is often a better predictor of real-world decode throughput/\\$ than TFLOPs/\\$.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) System-level implications\n",
        "\n",
        "### 5.1 VRAM limits concurrency\n",
        "\n",
        "Total KV memory scales with **sequence length × (batch or beams) × layers × KV heads**. At long contexts, KV can dominate VRAM. This caps:\n",
        "\n",
        "* How many requests you can run concurrently on one GPU\n",
        "* How wide you can set **beam search**\n",
        "\n",
        "### 5.2 Parallelism & placement\n",
        "\n",
        "* With **tensor parallelism**, KV is usually **sharded by heads** across GPUs. Good for memory, but be mindful of cross-GPU traffic.\n",
        "* **Pipeline parallelism** keeps layer shards on different GPUs; KV remains local per stage but you add pipeline bubbles.\n",
        "* **Data parallelism** doesn’t help KV size per GPU but affects gradient traffic (training).\n",
        "\n",
        "### 5.3 Serving architecture\n",
        "\n",
        "* **Paged KV / block allocators** avoid fragmentation and enable **continuous batching** (mixing requests at different decode steps).\n",
        "* **Prefix sharing/caching**: If multiple prompts share prefixes, reuse the same KV pages.\n",
        "* **Speculative decoding**: Can improve throughput and still leverage the same KV store for accepted tokens.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Practical ways to reduce KV pressure\n",
        "\n",
        "* **Architectural**\n",
        "\n",
        "  * **GQA/MQA**: Reduce KV heads dramatically.\n",
        "  * **Windowed/sliding attention**: Keep a moving context window; evict old tokens.\n",
        "  * **Sparse/block attention**: Read fewer K/V positions per step.\n",
        "\n",
        "* **Kernel-level**\n",
        "\n",
        "  * **Flash-Decoding / IO-aware kernels**: Tile to minimize HBM traffic and improve cache locality.\n",
        "  * **Fused ops** (e.g., attention + softmax + projections) to cut extra reads/writes.\n",
        "\n",
        "* **Compression**\n",
        "\n",
        "  * **KV quantization** (8-bit, 4-bit, NF4): Trade a little quality for big memory/bandwidth savings.\n",
        "  * **Mixed precision**: bf16/FP8 variations where supported.\n",
        "\n",
        "* **Scheduling**\n",
        "\n",
        "  * **Continuous batching** with smart admission control (avoid head-of-line blocking).\n",
        "  * **Latency classes**: Group similar sequence lengths to reduce worst-case cache reads.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Sizing worksheet\n",
        "\n",
        "### 7.1 KV memory footprint\n",
        "\n",
        "For given $L, n_{\\text{kv}}, d_{\\text{head}}, \\text{bytes}$, and target **context** $T$, **concurrency** $C$ (or beams), estimate:\n",
        "\n",
        "$$\n",
        "\\text{KV VRAM (GB)} \\approx\n",
        "\\frac{2 \\cdot L \\cdot n_{\\text{kv}} \\cdot d_{\\text{head}} \\cdot \\text{bytes} \\cdot T \\cdot C}{10^9}\n",
        "\\times \\phi\n",
        "$$\n",
        "\n",
        "$\\phi$ is an overhead factor (allocator fragmentation, metadata), typically **1.1–1.3** in practice.\n",
        "\n",
        "### 7.2 Required HBM bandwidth for a target TPS\n",
        "\n",
        "At average context $\\bar{t}$ (tokens already in cache), **per new token** reads roughly:\n",
        "\n",
        "$$\n",
        "\\text{Bytes/token} \\approx 2 \\cdot L \\cdot n_{\\text{kv}} \\cdot d_{\\text{head}} \\cdot \\text{bytes} \\cdot \\bar{t}\n",
        "$$\n",
        "\n",
        "For a target throughput $R$ tokens/s:\n",
        "\n",
        "$$\n",
        "\\text{HBM BW (GB/s)} \\approx \\frac{\\text{Bytes/token} \\cdot R}{10^9}\n",
        "$$\n",
        "\n",
        "Compare this with **usable** HBM GB/s of the GPU. If you exceed it, you’re memory-bound—reduce bytes/token or add GPUs.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Tiny helper: compute KV size & decode BW (Python)\n",
        "\n",
        "```python\n",
        "def kv_bytes_per_token(L, n_kv, d_head, bytes_per_elem=2):\n",
        "    return 2 * L * n_kv * d_head * bytes_per_elem  # bytes\n",
        "\n",
        "def kv_vram_gb(L, n_kv, d_head, T, concurrency, bytes_per_elem=2, overhead=1.2):\n",
        "    per_tok = kv_bytes_per_token(L, n_kv, d_head, bytes_per_elem)\n",
        "    return overhead * per_tok * T * concurrency / 1e9\n",
        "\n",
        "def decode_bandwidth_gbps(L, n_kv, d_head, avg_ctx, tps, bytes_per_elem=2):\n",
        "    per_tok_read = kv_bytes_per_token(L, n_kv, d_head, bytes_per_elem) * avg_ctx\n",
        "    return per_tok_read * tps * 8 / 1e9  # to Gb/s\n",
        "```\n",
        "\n",
        "**Try it:**\n",
        "\n",
        "* Standard MHA (64 KV heads) vs. GQA (8) at $L{=}80, d_{\\text{head}}{=}128$\n",
        "* Concurrency $= 16$, context $= 2{,}048$, TPS target $= 2000$, avg\\_ctx $≈ 1536$\n",
        "\n",
        "You’ll see why MHA quickly blows VRAM and HBM budgets, while GQA makes the same goal feasible.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) KV cache in **training** vs **inference**\n",
        "\n",
        "* **Inference (autoregressive decode):** KV cache is central; bottleneck is memory bandwidth.\n",
        "* **Standard full-sequence training:** You usually don’t keep a persistent KV cache across steps; attention runs over the whole sequence at once. Bottlenecks are mixed (compute + memory).\n",
        "* **Streaming/segment training** (e.g., recurrent or state-carrying variants): You *do* carry state reminiscent of KV across segments—plan memory/bandwidth similarly to inference.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Common pitfalls\n",
        "\n",
        "* **Underestimating VRAM:** KV scales with **context × concurrency × beams**; add a safety factor for allocator overhead.\n",
        "* **Ignoring topology:** In multi-GPU serving, ensure KV shards align with NVLink/NVSwitch; avoid tail-latency from cross-GPU reads on weak links.\n",
        "* **Only tracking TFLOPs:** For decode, monitor **HBM bandwidth** and **bytes/token**. TFLOPs alone won’t explain performance.\n",
        "* **Beam search blowups:** KV grows with beam width—plan for the worst case or cap beam size.\n",
        "\n",
        "---\n",
        "\n",
        "## 11) What to monitor in production\n",
        "\n",
        "* **Tokens/sec vs. context length** (watch the slope)\n",
        "* **HBM bandwidth utilization** (GPU profiler metrics)\n",
        "* **VRAM usage** split by weights / activations / KV\n",
        "* **Latency distribution** under mixed sequence lengths (P95/P99)\n",
        "* **Fragmentation** / page efficiency if you use a paged KV allocator\n",
        "* **Cross-GPU traffic** if sharding by heads (bytes/s, tail events)\n",
        "\n",
        "---\n",
        "\n",
        "## 12) Tying back to **cost-per-bandwidth**\n",
        "\n",
        "For memory-bound decode, rank hardware by:\n",
        "\n",
        "$$\n",
        "\\boxed{\\;\\$ / \\text{TB/s·hour} = \\frac{\\$ / \\text{GPU-hour}}{\\text{HBM TB/s per GPU}}\\;}\n",
        "$$\n",
        "\n",
        "Pick the SKU (and interconnect) that gives enough HBM **and** the lowest **\\$/TB/s·hr** while meeting your latency/QPS targets. Then push bytes/token down with GQA/MQA, windowed/sparse attention, IO-aware kernels, and KV quantization.\n",
        "\n",
        "---\n",
        "\n",
        "## 13) Summary\n",
        "\n",
        "The KV cache stores per-layer keys and values for all past tokens so an LLM can generate the next token by computing a fresh query and *looking up* past K/V rather than recomputing the entire prefix. It dramatically reduces compute but shifts the bottleneck to **memory**: cache size grows linearly with sequence length (and batch/beam), and decode becomes **HBM-bandwidth bound**. That’s why long-context serving cares more about **bytes moved** than TFLOPs. In practice we shrink and streamline KV with **GQA/MQA**, **paged KV**, **Flash-Decoding**, **windowed/sparse attention**, and **KV quantization**—and we size fleets using a **cost-per-memory-bandwidth** lens, not just TFLOPs/\\$.\n",
        "\n",
        "---\n",
        "\n",
        "### Final takeaway\n",
        "\n",
        "If your LLM serving doesn’t talk explicitly about **KV bytes/token**, **HBM GB/s**, **VRAM headroom**, and **\\$/TB/s·hr**, you’re flying blind. Make KV cache a first-class design dimension—from model choice (GQA/MQA) to kernels, parallelism, and cluster economics.\n"
      ],
      "metadata": {
        "id": "84XiFXX9InsA"
      }
    }
  ]
}